{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37fe2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import PyPDF2\n",
    "import re\n",
    "import numpy as np\n",
    "import openai\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f72495de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b8ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pdf files from a directory and extract text using PyPDF2\n",
    "# Define the class for reading PDF files\n",
    "\n",
    "class SimplePDFReader:\n",
    "    \"\"\"Class for reading PDF files from a directory.\"\"\"\n",
    "    \n",
    "    def read_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from a single PDF file.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            \n",
    "        Returns:\n",
    "            Extracted text from the PDF\n",
    "        \"\"\"\n",
    "        if not os.path.exists(pdf_path):\n",
    "            logger.error(f\"PDF file not found: {pdf_path}\")\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            text = \"\"\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                num_pages = len(pdf_reader.pages)\n",
    "                \n",
    "                logger.info(f\"Reading {pdf_path} with {num_pages} pages\")\n",
    "                \n",
    "                for page_num in range(num_pages):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                    \n",
    "                    # Log progress for large PDFs\n",
    "                    if num_pages > 20 and (page_num + 1) % 10 == 0:\n",
    "                        logger.info(f\"Progress: {page_num + 1}/{num_pages} pages processed\")\n",
    "            \n",
    "            # Clean the text\n",
    "            text = self._clean_text(text)\n",
    "            logger.info(f\"Successfully extracted {len(text)} characters from {pdf_path}\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading PDF {pdf_path}: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean the extracted text.\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def read_pdfs_from_directory(self, directory_path: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Read all PDFs from a directory.\n",
    "        \n",
    "        Args:\n",
    "            directory_path: Path to directory containing PDFs\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping filenames to their extracted text content\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(directory_path):\n",
    "            logger.error(f\"Directory not found: {directory_path}\")\n",
    "            return {}\n",
    "        \n",
    "        pdf_contents = {}\n",
    "        pdf_files = [f for f in os.listdir(directory_path) if f.lower().endswith('.pdf')]\n",
    "        \n",
    "        if not pdf_files:\n",
    "            logger.warning(f\"No PDF files found in {directory_path}\")\n",
    "            return {}\n",
    "        \n",
    "        logger.info(f\"Found {len(pdf_files)} PDF files in {directory_path}\")\n",
    "        \n",
    "        for i, pdf_file in enumerate(pdf_files):\n",
    "            pdf_path = os.path.join(directory_path, pdf_file)\n",
    "            logger.info(f\"Processing PDF {i+1}/{len(pdf_files)}: {pdf_file}\")\n",
    "            \n",
    "            text = self.read_pdf(pdf_path)\n",
    "            if text:\n",
    "                pdf_contents[pdf_file] = text\n",
    "            else:\n",
    "                logger.warning(f\"No text was extracted from {pdf_file}\")\n",
    "        \n",
    "        successful = sum(1 for text in pdf_contents.values() if text)\n",
    "        logger.info(f\"Successfully processed {successful} out of {len(pdf_files)} PDF files\")\n",
    "        \n",
    "        return pdf_contents\n",
    "    \n",
    "    def save_extracted_text(self, pdf_contents: Dict[str, str], output_dir: str) -> None:\n",
    "        \"\"\"\n",
    "        Save extracted text from PDFs to individual text files.\n",
    "        \n",
    "        Args:\n",
    "            pdf_contents: Dictionary mapping filenames to their extracted text\n",
    "            output_dir: Directory where text files will be saved\n",
    "        \"\"\"\n",
    "        if not pdf_contents:\n",
    "            logger.warning(\"No PDF contents to save\")\n",
    "            return\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for pdf_file, text in pdf_contents.items():\n",
    "            # Create an output filename by replacing .pdf extension with .txt\n",
    "            output_filename = os.path.splitext(pdf_file)[0] + \".txt\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            \n",
    "            try:\n",
    "                with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(text)\n",
    "                logger.info(f\"Saved extracted text to {output_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving text for {pdf_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3056aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:07:03,160 - INFO - Found 34 PDF files in \\Users\\yyy04\\Downloads\\New folder\\data\n",
      "2025-05-07 10:07:03,160 - INFO - Processing PDF 1/34: III100(A).pdf\n",
      "2025-05-07 10:07:03,175 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III100(A).pdf with 16 pages\n",
      "2025-05-07 10:07:03,384 - INFO - Successfully extracted 61630 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III100(A).pdf\n",
      "2025-05-07 10:07:03,385 - INFO - Processing PDF 2/34: III100(B).pdf\n",
      "2025-05-07 10:07:03,397 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III100(B).pdf with 1 pages\n",
      "2025-05-07 10:07:03,408 - INFO - Successfully extracted 1816 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III100(B).pdf\n",
      "2025-05-07 10:07:03,409 - INFO - Processing PDF 3/34: III100(C).pdf\n",
      "2025-05-07 10:07:03,419 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III100(C).pdf with 1 pages\n",
      "2025-05-07 10:07:03,430 - INFO - Successfully extracted 2123 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III100(C).pdf\n",
      "2025-05-07 10:07:03,431 - INFO - Processing PDF 4/34: III100.pdf\n",
      "2025-05-07 10:07:03,441 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III100.pdf with 5 pages\n",
      "2025-05-07 10:07:03,509 - INFO - Successfully extracted 10081 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III100.pdf\n",
      "2025-05-07 10:07:03,509 - INFO - Processing PDF 5/34: III110.pdf\n",
      "2025-05-07 10:07:03,520 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III110.pdf with 6 pages\n",
      "2025-05-07 10:07:03,547 - INFO - Successfully extracted 14588 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III110.pdf\n",
      "2025-05-07 10:07:03,548 - INFO - Processing PDF 6/34: III110A.pdf\n",
      "2025-05-07 10:07:03,572 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III110A.pdf with 36 pages\n",
      "2025-05-07 10:07:03,756 - INFO - Progress: 10/36 pages processed\n",
      "2025-05-07 10:07:03,878 - INFO - Progress: 20/36 pages processed\n",
      "2025-05-07 10:07:04,065 - INFO - Progress: 30/36 pages processed\n",
      "2025-05-07 10:07:04,140 - INFO - Successfully extracted 99318 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III110A.pdf\n",
      "2025-05-07 10:07:04,141 - INFO - Processing PDF 7/34: III111.pdf\n",
      "2025-05-07 10:07:04,153 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III111.pdf with 3 pages\n",
      "2025-05-07 10:07:04,225 - INFO - Successfully extracted 7868 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III111.pdf\n",
      "2025-05-07 10:07:04,226 - INFO - Processing PDF 8/34: III120(A).pdf\n",
      "2025-05-07 10:07:04,237 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III120(A).pdf with 3 pages\n",
      "2025-05-07 10:07:04,264 - INFO - Successfully extracted 5794 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III120(A).pdf\n",
      "2025-05-07 10:07:04,265 - INFO - Processing PDF 9/34: III120(B).pdf\n",
      "2025-05-07 10:07:04,274 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III120(B).pdf with 3 pages\n",
      "2025-05-07 10:07:04,354 - INFO - Successfully extracted 9737 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III120(B).pdf\n",
      "2025-05-07 10:07:04,354 - INFO - Processing PDF 10/34: III130(A).pdf\n",
      "2025-05-07 10:07:04,364 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III130(A).pdf with 1 pages\n",
      "2025-05-07 10:07:04,377 - INFO - Successfully extracted 2223 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III130(A).pdf\n",
      "2025-05-07 10:07:04,378 - INFO - Processing PDF 11/34: III141(A).pdf\n",
      "2025-05-07 10:07:04,388 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III141(A).pdf with 3 pages\n",
      "2025-05-07 10:07:04,415 - INFO - Successfully extracted 7613 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III141(A).pdf\n",
      "2025-05-07 10:07:04,416 - INFO - Processing PDF 12/34: III141.pdf\n",
      "2025-05-07 10:07:04,426 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III141.pdf with 1 pages\n",
      "2025-05-07 10:07:04,434 - INFO - Successfully extracted 2106 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III141.pdf\n",
      "2025-05-07 10:07:04,435 - INFO - Processing PDF 13/34: III150(A).pdf\n",
      "2025-05-07 10:07:04,444 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III150(A).pdf with 2 pages\n",
      "2025-05-07 10:07:04,466 - INFO - Successfully extracted 5866 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III150(A).pdf\n",
      "2025-05-07 10:07:04,466 - INFO - Processing PDF 14/34: III220(A).pdf\n",
      "2025-05-07 10:07:04,476 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III220(A).pdf with 2 pages\n",
      "2025-05-07 10:07:04,498 - INFO - Successfully extracted 5989 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III220(A).pdf\n",
      "2025-05-07 10:07:04,499 - INFO - Processing PDF 15/34: III240(A).pdf\n",
      "2025-05-07 10:07:04,508 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III240(A).pdf with 2 pages\n",
      "2025-05-07 10:07:04,528 - INFO - Successfully extracted 4222 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III240(A).pdf\n",
      "2025-05-07 10:07:04,528 - INFO - Processing PDF 16/34: III240.pdf\n",
      "2025-05-07 10:07:04,540 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III240.pdf with 2 pages\n",
      "2025-05-07 10:07:04,551 - INFO - Successfully extracted 3233 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III240.pdf\n",
      "2025-05-07 10:07:04,552 - INFO - Processing PDF 17/34: III241.pdf\n",
      "2025-05-07 10:07:04,561 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III241.pdf with 2 pages\n",
      "2025-05-07 10:07:04,572 - INFO - Successfully extracted 4602 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III241.pdf\n",
      "2025-05-07 10:07:04,573 - INFO - Processing PDF 18/34: III250(A).pdf\n",
      "2025-05-07 10:07:04,583 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III250(A).pdf with 2 pages\n",
      "2025-05-07 10:07:04,602 - INFO - Successfully extracted 3398 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III250(A).pdf\n",
      "2025-05-07 10:07:04,602 - INFO - Processing PDF 19/34: III300(A).pdf\n",
      "2025-05-07 10:07:04,612 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III300(A).pdf with 1 pages\n",
      "2025-05-07 10:07:04,626 - INFO - Successfully extracted 3218 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III300(A).pdf\n",
      "2025-05-07 10:07:04,626 - INFO - Processing PDF 20/34: III300.pdf\n",
      "2025-05-07 10:07:04,634 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III300.pdf with 2 pages\n",
      "2025-05-07 10:07:04,659 - INFO - Successfully extracted 3870 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III300.pdf\n",
      "2025-05-07 10:07:04,661 - INFO - Processing PDF 21/34: III400(C).pdf\n",
      "2025-05-07 10:07:04,670 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III400(C).pdf with 3 pages\n",
      "2025-05-07 10:07:04,693 - INFO - Successfully extracted 6069 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III400(C).pdf\n",
      "2025-05-07 10:07:04,694 - INFO - Processing PDF 22/34: III400.pdf\n",
      "2025-05-07 10:07:04,703 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III400.pdf with 7 pages\n",
      "2025-05-07 10:07:04,811 - INFO - Successfully extracted 16088 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III400.pdf\n",
      "2025-05-07 10:07:04,812 - INFO - Processing PDF 23/34: III510.pdf\n",
      "2025-05-07 10:07:04,821 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III510.pdf with 1 pages\n",
      "2025-05-07 10:07:04,845 - INFO - Successfully extracted 2865 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III510.pdf\n",
      "2025-05-07 10:07:04,846 - INFO - Processing PDF 24/34: III600(A).pdf\n",
      "2025-05-07 10:07:04,856 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III600(A).pdf with 1 pages\n",
      "2025-05-07 10:07:04,905 - INFO - Successfully extracted 1203 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III600(A).pdf\n",
      "2025-05-07 10:07:04,905 - INFO - Processing PDF 25/34: III600(B).pdf\n",
      "2025-05-07 10:07:04,915 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III600(B).pdf with 1 pages\n",
      "2025-05-07 10:07:04,928 - INFO - Successfully extracted 1848 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III600(B).pdf\n",
      "2025-05-07 10:07:04,928 - INFO - Processing PDF 26/34: III600.pdf\n",
      "2025-05-07 10:07:04,938 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III600.pdf with 1 pages\n",
      "2025-05-07 10:07:04,951 - INFO - Successfully extracted 1943 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III600.pdf\n",
      "2025-05-07 10:07:04,951 - INFO - Processing PDF 27/34: III610.pdf\n",
      "2025-05-07 10:07:04,960 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III610.pdf with 1 pages\n",
      "2025-05-07 10:07:04,964 - INFO - Successfully extracted 731 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III610.pdf\n",
      "2025-05-07 10:07:04,965 - INFO - Processing PDF 28/34: III620(A).pdf\n",
      "2025-05-07 10:07:04,975 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III620(A).pdf with 2 pages\n",
      "2025-05-07 10:07:04,997 - INFO - Successfully extracted 4249 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III620(A).pdf\n",
      "2025-05-07 10:07:04,997 - INFO - Processing PDF 29/34: III620(B).pdf\n",
      "2025-05-07 10:07:05,006 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III620(B).pdf with 1 pages\n",
      "2025-05-07 10:07:05,020 - INFO - Successfully extracted 3088 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III620(B).pdf\n",
      "2025-05-07 10:07:05,021 - INFO - Processing PDF 30/34: III630(A).pdf\n",
      "2025-05-07 10:07:05,032 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III630(A).pdf with 7 pages\n",
      "2025-05-07 10:07:05,109 - INFO - Successfully extracted 22062 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III630(A).pdf\n",
      "2025-05-07 10:07:05,109 - INFO - Processing PDF 31/34: III640(A).pdf\n",
      "2025-05-07 10:07:05,123 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III640(A).pdf with 1 pages\n",
      "2025-05-07 10:07:05,133 - INFO - Successfully extracted 824 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III640(A).pdf\n",
      "2025-05-07 10:07:05,133 - INFO - Processing PDF 32/34: III700(A).pdf\n",
      "2025-05-07 10:07:05,144 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III700(A).pdf with 4 pages\n",
      "2025-05-07 10:07:05,184 - INFO - Successfully extracted 12675 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III700(A).pdf\n",
      "2025-05-07 10:07:05,185 - INFO - Processing PDF 33/34: III800(A).pdf\n",
      "2025-05-07 10:07:05,195 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III800(A).pdf with 1 pages\n",
      "2025-05-07 10:07:05,210 - INFO - Successfully extracted 4219 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III800(A).pdf\n",
      "2025-05-07 10:07:05,211 - INFO - Processing PDF 34/34: III900(A).pdf\n",
      "2025-05-07 10:07:05,221 - INFO - Reading \\Users\\yyy04\\Downloads\\New folder\\data\\III900(A).pdf with 2 pages\n",
      "2025-05-07 10:07:05,241 - INFO - Successfully extracted 6226 characters from \\Users\\yyy04\\Downloads\\New folder\\data\\III900(A).pdf\n",
      "2025-05-07 10:07:05,242 - INFO - Successfully processed 34 out of 34 PDF files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "III100(A).pdf: 61630 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III100(B).pdf: 1816 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III100(C).pdf: 2123 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III100.pdf: 10081 characters, sample: USM Bylaws, Policies and Procedures of the Board of Regents III-1.00-1 III-1.00 - POLICY ON ACADEMIC...\n",
      "III110.pdf: 14588 characters, sample: USM Bylaws, Policies and Procedures of the Board of Regents UNIVERSITY SYSTEM OF MARYLAND III-1.10 -...\n",
      "III110A.pdf: 99318 characters, sample: III-1.10(A) UNIVERSITY OF MARYLAND POLICY AND PROCEDURES CONCERNING SCHOLARLY MISCONDUCT (Approved b...\n",
      "III111.pdf: 7868 characters, sample: USM Bylaws, Policies and Procedures of the Board of Regents III‐1.11 POLICY ON CONFLICTS OF INTEREST...\n",
      "III120(A).pdf: 5794 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III120(B).pdf: 9737 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III130(A).pdf: 2223 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III141(A).pdf: 7613 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III141.pdf: 2106 characters, sample: USM Bylaws, Policies and Procedures of the Board of Regents UNIVERSITY SYSTEM OF MARYLAND III-1.41 –...\n",
      "III150(A).pdf: 5866 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III220(A).pdf: 5989 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III240(A).pdf: 4222 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III240.pdf: 3233 characters, sample: USM Bylaws, Policies and Procedures of the Board of Regents UNIVERSITY SYSTEM OF MARYLAND III-2.40 -...\n",
      "III241.pdf: 4602 characters, sample: USM Bylaws, Policies and Procedures of the Board of Regents UNIVERSITY SYSTEM OF MARYLAND III-2.41 -...\n",
      "III250(A).pdf: 3398 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III300(A).pdf: 3218 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III300.pdf: 3870 characters, sample: USM Bylaws, Policies and Procedures of the Board of Regents III-3.00 – 1 III-3.00 – POLICY ON THE AW...\n",
      "III400(C).pdf: 6069 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III400.pdf: 16088 characters, sample: USM Bylaws, Policies, and Procedures of the Board of Regents III-4.00 - POLICY ON UNDERGRADUATE ADMI...\n",
      "III510.pdf: 2865 characters, sample: USM Bylaws, Policies and Procedures of the Board of Regents III-5.10 III-5.10-POLICY CONCERNING THE ...\n",
      "III600(A).pdf: 1203 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III600(B).pdf: 1848 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III600.pdf: 1943 characters, sample: USM Bylaws, Policies and Procedures of the Board of Regents III-6.00 - POLICY ON ACADEMIC TRANSCRIPT...\n",
      "III610.pdf: 731 characters, sample: USM Bylaws, Policies and Procedures of the Board of Regents UNIVERSITY SYSTEM OF MARYLAND III-6.10 -...\n",
      "III620(A).pdf: 4249 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III620(B).pdf: 3088 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III630(A).pdf: 22062 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III640(A).pdf: 824 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III700(A).pdf: 12675 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III800(A).pdf: 4219 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n",
      "III900(A).pdf: 6226 characters, sample: University Policies [ Section III: Academic Affairs ](https://policies.umd.edu/academic-affairs) Pol...\n"
     ]
    }
   ],
   "source": [
    "# Run the PDF reader\n",
    "\n",
    "reader = SimplePDFReader()\n",
    "pdf_directory = r\"\\Users\\yyy04\\Downloads\\New folder\\data\"\n",
    "\n",
    "pdf_contents = reader.read_pdfs_from_directory(pdf_directory)\n",
    "\n",
    "# Print a summary of what was read\n",
    "for filename, content in pdf_contents.items():\n",
    "    print(f\"{filename}: {len(content)} characters, sample: {content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bf3eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Required libraries for embeddings and ChromaDB\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "api_key = \"***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a1efa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "\n",
    "class PDFEmbeddingPipeline:\n",
    "    \"\"\"Pipeline for reading PDFs, generating embeddings, and storing in ChromaDB.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        openai_api_key: str,\n",
    "        chroma_db_path: str = \"./chroma_db\",\n",
    "        collection_name: str = \"pdf_embeddings\",\n",
    "        embedding_model: str = \"text-embedding-ada-002\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the PDF to Embeddings pipeline.\n",
    "        \n",
    "        Args:\n",
    "            openai_api_key: OpenAI API key for embeddings\n",
    "            chroma_db_path: Directory to store ChromaDB\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            embedding_model: OpenAI embedding model name\n",
    "            chunk_size: Size of text chunks\n",
    "            chunk_overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "        self.openai_api_key = openai_api_key\n",
    "        openai.api_key = openai_api_key\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # Initialize ChromaDB\n",
    "        self.chroma_client = chromadb.PersistentClient(path=chroma_db_path)\n",
    "        \n",
    "        # Create or get the collection\n",
    "        openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "            api_key=openai_api_key,\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        \n",
    "    \n",
    "        try:\n",
    "            self.collection = self.chroma_client.get_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=openai_ef\n",
    "            )\n",
    "            logger.info(f\"Connected to existing collection: {collection_name}\")\n",
    "        except ValueError:\n",
    "            self.collection = self.chroma_client.create_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=openai_ef\n",
    "            )\n",
    "            logger.info(f\"Created new collection: {collection_name}\")\n",
    "    \n",
    "    def read_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from a PDF file.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            \n",
    "        Returns:\n",
    "            Extracted text from the PDF\n",
    "        \"\"\"\n",
    "        if not os.path.exists(pdf_path):\n",
    "            logger.error(f\"PDF file not found: {pdf_path}\")\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            text = \"\"\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                num_pages = len(pdf_reader.pages)\n",
    "                \n",
    "                logger.info(f\"Reading {pdf_path} with {num_pages} pages\")\n",
    "                \n",
    "                for page_num in range(num_pages):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                    \n",
    "                    # Log progress for large PDFs\n",
    "                    if num_pages > 20 and (page_num + 1) % 10 == 0:\n",
    "                        logger.info(f\"Progress: {page_num + 1}/{num_pages} pages processed\")\n",
    "            \n",
    "            # Clean the text\n",
    "            text = self._clean_text(text)\n",
    "            logger.info(f\"Successfully extracted {len(text)} characters from {pdf_path}\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading PDF {pdf_path}: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean the extracted text.\"\"\"\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def chunk_text(self, text: str, metadata: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Split text into chunks with specified overlap and add metadata.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to chunk\n",
    "            metadata: Metadata for the document (e.g., filename, path)\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with text chunks and metadata\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            \n",
    "            # If we're not at the end of the text, try to break at a logical point\n",
    "            if end < len(text):\n",
    "                # Look for good breakpoints: paragraph, sentence, or word\n",
    "                paragraph_break = text.rfind('\\n', start, end)\n",
    "                sentence_break = text.rfind('. ', start, end)\n",
    "                space_break = text.rfind(' ', start, end)\n",
    "                \n",
    "                if paragraph_break != -1 and paragraph_break > start + self.chunk_size // 2:\n",
    "                    end = paragraph_break + 1\n",
    "                elif sentence_break != -1 and sentence_break > start + self.chunk_size // 2:\n",
    "                    end = sentence_break + 2\n",
    "                elif space_break != -1:\n",
    "                    end = space_break + 1\n",
    "            \n",
    "            chunk_text = text[start:end].strip()\n",
    "            if chunk_text:\n",
    "                # Create a unique ID for this chunk\n",
    "                unique_id = f\"{metadata.get('filename', 'doc')}_{chunk_id}\"\n",
    "                \n",
    "                # Create chunk with metadata\n",
    "                chunk = {\n",
    "                    \"id\": unique_id,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"metadata\": {\n",
    "                        **metadata,\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"start_char\": start,\n",
    "                        \"end_char\": end\n",
    "                    }\n",
    "                }\n",
    "                chunks.append(chunk)\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # Move the start position, accounting for overlap\n",
    "            start = end - self.chunk_overlap if end - self.chunk_overlap > start else end\n",
    "        \n",
    "        logger.info(f\"Created {len(chunks)} chunks from text\")\n",
    "        return chunks\n",
    "    \n",
    "    def add_to_chroma(self, chunks: List[Dict[str, Any]]) -> None:\n",
    "        \"\"\"\n",
    "        Add chunks to ChromaDB.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of dictionaries with text chunks and metadata\n",
    "        \"\"\"\n",
    "        if not chunks:\n",
    "            logger.warning(\"No chunks to add to ChromaDB\")\n",
    "            return\n",
    "        \n",
    "        batch_size = 20  # Process in batches to avoid rate limits\n",
    "        total_added = 0\n",
    "        \n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Extract data for ChromaDB\n",
    "                ids = [chunk[\"id\"] for chunk in batch]\n",
    "                texts = [chunk[\"text\"] for chunk in batch]\n",
    "                metadatas = [chunk[\"metadata\"] for chunk in batch]\n",
    "                \n",
    "                # Add documents to ChromaDB\n",
    "                self.collection.add(\n",
    "                    ids=ids,\n",
    "                    documents=texts,\n",
    "                    metadatas=metadatas\n",
    "                )\n",
    "                \n",
    "                total_added += len(batch)\n",
    "                logger.info(f\"Added batch {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1} to ChromaDB\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error adding batch to ChromaDB: {str(e)}\")\n",
    "            \n",
    "            # Add a small delay to avoid rate limiting\n",
    "            if i + batch_size < len(chunks):\n",
    "                time.sleep(0.5)\n",
    "        \n",
    "        logger.info(f\"Successfully added {total_added} chunks to ChromaDB\")\n",
    "    \n",
    "    def process_pdf(self, pdf_path: str) -> int:\n",
    "        \"\"\"\n",
    "        Process a PDF file: read, chunk, and add to ChromaDB.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            \n",
    "        Returns:\n",
    "            Number of chunks added to ChromaDB\n",
    "        \"\"\"\n",
    "        # Extract filename from path\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        \n",
    "        # Create metadata for this PDF\n",
    "        metadata = {\n",
    "            \"filename\": filename,\n",
    "            \"source_path\": pdf_path,\n",
    "            \"processed_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "        \n",
    "        # Read PDF\n",
    "        text = self.read_pdf(pdf_path)\n",
    "        if not text:\n",
    "            logger.warning(f\"No text extracted from {pdf_path}\")\n",
    "            return 0\n",
    "        \n",
    "        # Chunk the text\n",
    "        chunks = self.chunk_text(text, metadata)\n",
    "        if not chunks:\n",
    "            logger.warning(f\"No chunks created from {pdf_path}\")\n",
    "            return 0\n",
    "        \n",
    "        # Add chunks to ChromaDB\n",
    "        self.add_to_chroma(chunks)\n",
    "        \n",
    "        return len(chunks)\n",
    "    \n",
    "    def process_directory(self, directory_path: str) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Process all PDFs in a directory.\n",
    "        \n",
    "        Args:\n",
    "            directory_path: Path to directory containing PDFs\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping filenames to number of chunks added\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(directory_path):\n",
    "            logger.error(f\"Directory not found: {directory_path}\")\n",
    "            return {}\n",
    "        \n",
    "        results = {}\n",
    "        pdf_files = [f for f in os.listdir(directory_path) if f.lower().endswith('.pdf')]\n",
    "        \n",
    "        if not pdf_files:\n",
    "            logger.warning(f\"No PDF files found in {directory_path}\")\n",
    "            return {}\n",
    "        \n",
    "        logger.info(f\"Found {len(pdf_files)} PDF files in {directory_path}\")\n",
    "        \n",
    "        for i, pdf_file in enumerate(pdf_files):\n",
    "            pdf_path = os.path.join(directory_path, pdf_file)\n",
    "            logger.info(f\"Processing PDF {i+1}/{len(pdf_files)}: {pdf_file}\")\n",
    "            \n",
    "            chunks_added = self.process_pdf(pdf_path)\n",
    "            results[pdf_file] = chunks_added\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def query(self, query_text: str, n_results: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Query the ChromaDB for similar chunks.\n",
    "        \n",
    "        Args:\n",
    "            query_text: Text to search for\n",
    "            n_results: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with query results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self.collection.query(\n",
    "                query_texts=[query_text],\n",
    "                n_results=n_results\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error querying ChromaDB: {str(e)}\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b511807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 10:36:06,095 - INFO - Connected to existing collection: pdf_embeddings\n",
      "2025-05-07 10:36:06,096 - INFO - Found 34 PDF files in ./data\n",
      "2025-05-07 10:36:06,096 - INFO - Processing PDF 1/34: III100(A).pdf\n",
      "2025-05-07 10:36:06,109 - INFO - Reading ./data\\III100(A).pdf with 16 pages\n",
      "2025-05-07 10:36:06,284 - INFO - Successfully extracted 61630 characters from ./data\\III100(A).pdf\n",
      "2025-05-07 10:36:06,285 - INFO - Created 91 chunks from text\n",
      "2025-05-07 10:36:07,085 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:07,229 - INFO - Added batch 1/5 to ChromaDB\n",
      "2025-05-07 10:36:08,406 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:08,573 - INFO - Added batch 2/5 to ChromaDB\n",
      "2025-05-07 10:36:09,257 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:09,303 - INFO - Added batch 3/5 to ChromaDB\n",
      "2025-05-07 10:36:10,864 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:11,110 - INFO - Added batch 4/5 to ChromaDB\n",
      "2025-05-07 10:36:12,348 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:12,385 - INFO - Added batch 5/5 to ChromaDB\n",
      "2025-05-07 10:36:12,385 - INFO - Successfully added 91 chunks to ChromaDB\n",
      "2025-05-07 10:36:12,386 - INFO - Processing PDF 2/34: III100(B).pdf\n",
      "2025-05-07 10:36:12,396 - INFO - Reading ./data\\III100(B).pdf with 1 pages\n",
      "2025-05-07 10:36:12,407 - INFO - Successfully extracted 1816 characters from ./data\\III100(B).pdf\n",
      "2025-05-07 10:36:12,408 - INFO - Created 4 chunks from text\n",
      "2025-05-07 10:36:13,231 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:13,315 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:13,315 - INFO - Successfully added 4 chunks to ChromaDB\n",
      "2025-05-07 10:36:13,315 - INFO - Processing PDF 3/34: III100(C).pdf\n",
      "2025-05-07 10:36:13,326 - INFO - Reading ./data\\III100(C).pdf with 1 pages\n",
      "2025-05-07 10:36:13,338 - INFO - Successfully extracted 2123 characters from ./data\\III100(C).pdf\n",
      "2025-05-07 10:36:13,339 - INFO - Created 4 chunks from text\n",
      "2025-05-07 10:36:14,104 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:14,258 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:14,259 - INFO - Successfully added 4 chunks to ChromaDB\n",
      "2025-05-07 10:36:14,259 - INFO - Processing PDF 4/34: III100.pdf\n",
      "2025-05-07 10:36:14,269 - INFO - Reading ./data\\III100.pdf with 5 pages\n",
      "2025-05-07 10:36:14,339 - INFO - Successfully extracted 10081 characters from ./data\\III100.pdf\n",
      "2025-05-07 10:36:14,339 - INFO - Created 16 chunks from text\n",
      "2025-05-07 10:36:15,329 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:15,434 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:15,434 - INFO - Successfully added 16 chunks to ChromaDB\n",
      "2025-05-07 10:36:15,436 - INFO - Processing PDF 5/34: III110.pdf\n",
      "2025-05-07 10:36:15,446 - INFO - Reading ./data\\III110.pdf with 6 pages\n",
      "2025-05-07 10:36:15,474 - INFO - Successfully extracted 14588 characters from ./data\\III110.pdf\n",
      "2025-05-07 10:36:15,475 - INFO - Created 22 chunks from text\n",
      "2025-05-07 10:36:16,453 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:16,709 - INFO - Added batch 1/2 to ChromaDB\n",
      "2025-05-07 10:36:17,758 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:17,788 - INFO - Added batch 2/2 to ChromaDB\n",
      "2025-05-07 10:36:17,789 - INFO - Successfully added 22 chunks to ChromaDB\n",
      "2025-05-07 10:36:17,789 - INFO - Processing PDF 6/34: III110A.pdf\n",
      "2025-05-07 10:36:17,810 - INFO - Reading ./data\\III110A.pdf with 36 pages\n",
      "2025-05-07 10:36:18,025 - INFO - Progress: 10/36 pages processed\n",
      "2025-05-07 10:36:18,139 - INFO - Progress: 20/36 pages processed\n",
      "2025-05-07 10:36:18,331 - INFO - Progress: 30/36 pages processed\n",
      "2025-05-07 10:36:18,400 - INFO - Successfully extracted 99318 characters from ./data\\III110A.pdf\n",
      "2025-05-07 10:36:18,401 - INFO - Created 146 chunks from text\n",
      "2025-05-07 10:36:18,609 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:18,694 - INFO - Added batch 1/8 to ChromaDB\n",
      "2025-05-07 10:36:20,187 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:20,278 - INFO - Added batch 2/8 to ChromaDB\n",
      "2025-05-07 10:36:21,400 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:21,448 - INFO - Added batch 3/8 to ChromaDB\n",
      "2025-05-07 10:36:22,156 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:22,210 - INFO - Added batch 4/8 to ChromaDB\n",
      "2025-05-07 10:36:23,059 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:23,101 - INFO - Added batch 5/8 to ChromaDB\n",
      "2025-05-07 10:36:23,934 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:24,195 - INFO - Added batch 6/8 to ChromaDB\n",
      "2025-05-07 10:36:25,430 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:25,689 - INFO - Added batch 7/8 to ChromaDB\n",
      "2025-05-07 10:36:26,858 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:26,885 - INFO - Added batch 8/8 to ChromaDB\n",
      "2025-05-07 10:36:26,885 - INFO - Successfully added 146 chunks to ChromaDB\n",
      "2025-05-07 10:36:26,886 - INFO - Processing PDF 7/34: III111.pdf\n",
      "2025-05-07 10:36:26,898 - INFO - Reading ./data\\III111.pdf with 3 pages\n",
      "2025-05-07 10:36:26,964 - INFO - Successfully extracted 7868 characters from ./data\\III111.pdf\n",
      "2025-05-07 10:36:26,965 - INFO - Created 13 chunks from text\n",
      "2025-05-07 10:36:27,134 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:27,181 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:27,182 - INFO - Successfully added 13 chunks to ChromaDB\n",
      "2025-05-07 10:36:27,182 - INFO - Processing PDF 8/34: III120(A).pdf\n",
      "2025-05-07 10:36:27,191 - INFO - Reading ./data\\III120(A).pdf with 3 pages\n",
      "2025-05-07 10:36:27,216 - INFO - Successfully extracted 5794 characters from ./data\\III120(A).pdf\n",
      "2025-05-07 10:36:27,217 - INFO - Created 9 chunks from text\n",
      "2025-05-07 10:36:27,488 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:27,602 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:27,602 - INFO - Successfully added 9 chunks to ChromaDB\n",
      "2025-05-07 10:36:27,603 - INFO - Processing PDF 9/34: III120(B).pdf\n",
      "2025-05-07 10:36:27,612 - INFO - Reading ./data\\III120(B).pdf with 3 pages\n",
      "2025-05-07 10:36:27,645 - INFO - Successfully extracted 9737 characters from ./data\\III120(B).pdf\n",
      "2025-05-07 10:36:27,645 - INFO - Created 14 chunks from text\n",
      "2025-05-07 10:36:28,045 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:28,082 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:28,082 - INFO - Successfully added 14 chunks to ChromaDB\n",
      "2025-05-07 10:36:28,083 - INFO - Processing PDF 10/34: III130(A).pdf\n",
      "2025-05-07 10:36:28,091 - INFO - Reading ./data\\III130(A).pdf with 1 pages\n",
      "2025-05-07 10:36:28,103 - INFO - Successfully extracted 2223 characters from ./data\\III130(A).pdf\n",
      "2025-05-07 10:36:28,104 - INFO - Created 4 chunks from text\n",
      "2025-05-07 10:36:28,358 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:28,395 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:28,395 - INFO - Successfully added 4 chunks to ChromaDB\n",
      "2025-05-07 10:36:28,396 - INFO - Processing PDF 11/34: III141(A).pdf\n",
      "2025-05-07 10:36:28,405 - INFO - Reading ./data\\III141(A).pdf with 3 pages\n",
      "2025-05-07 10:36:28,433 - INFO - Successfully extracted 7613 characters from ./data\\III141(A).pdf\n",
      "2025-05-07 10:36:28,434 - INFO - Created 12 chunks from text\n",
      "2025-05-07 10:36:28,626 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:28,673 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:28,673 - INFO - Successfully added 12 chunks to ChromaDB\n",
      "2025-05-07 10:36:28,674 - INFO - Processing PDF 12/34: III141.pdf\n",
      "2025-05-07 10:36:28,683 - INFO - Reading ./data\\III141.pdf with 1 pages\n",
      "2025-05-07 10:36:28,692 - INFO - Successfully extracted 2106 characters from ./data\\III141.pdf\n",
      "2025-05-07 10:36:28,693 - INFO - Created 4 chunks from text\n",
      "2025-05-07 10:36:31,488 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:31,516 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:31,517 - INFO - Successfully added 4 chunks to ChromaDB\n",
      "2025-05-07 10:36:31,517 - INFO - Processing PDF 13/34: III150(A).pdf\n",
      "2025-05-07 10:36:31,526 - INFO - Reading ./data\\III150(A).pdf with 2 pages\n",
      "2025-05-07 10:36:31,548 - INFO - Successfully extracted 5866 characters from ./data\\III150(A).pdf\n",
      "2025-05-07 10:36:31,548 - INFO - Created 10 chunks from text\n",
      "2025-05-07 10:36:31,873 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:31,908 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:31,909 - INFO - Successfully added 10 chunks to ChromaDB\n",
      "2025-05-07 10:36:31,909 - INFO - Processing PDF 14/34: III220(A).pdf\n",
      "2025-05-07 10:36:31,918 - INFO - Reading ./data\\III220(A).pdf with 2 pages\n",
      "2025-05-07 10:36:31,941 - INFO - Successfully extracted 5989 characters from ./data\\III220(A).pdf\n",
      "2025-05-07 10:36:31,942 - INFO - Created 11 chunks from text\n",
      "2025-05-07 10:36:32,384 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:32,428 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:32,428 - INFO - Successfully added 11 chunks to ChromaDB\n",
      "2025-05-07 10:36:32,429 - INFO - Processing PDF 15/34: III240(A).pdf\n",
      "2025-05-07 10:36:32,437 - INFO - Reading ./data\\III240(A).pdf with 2 pages\n",
      "2025-05-07 10:36:32,455 - INFO - Successfully extracted 4222 characters from ./data\\III240(A).pdf\n",
      "2025-05-07 10:36:32,456 - INFO - Created 7 chunks from text\n",
      "2025-05-07 10:36:32,746 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:32,798 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:32,798 - INFO - Successfully added 7 chunks to ChromaDB\n",
      "2025-05-07 10:36:32,798 - INFO - Processing PDF 16/34: III240.pdf\n",
      "2025-05-07 10:36:32,808 - INFO - Reading ./data\\III240.pdf with 2 pages\n",
      "2025-05-07 10:36:32,818 - INFO - Successfully extracted 3233 characters from ./data\\III240.pdf\n",
      "2025-05-07 10:36:32,819 - INFO - Created 5 chunks from text\n",
      "2025-05-07 10:36:33,103 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:33,142 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:33,143 - INFO - Successfully added 5 chunks to ChromaDB\n",
      "2025-05-07 10:36:33,143 - INFO - Processing PDF 17/34: III241.pdf\n",
      "2025-05-07 10:36:33,152 - INFO - Reading ./data\\III241.pdf with 2 pages\n",
      "2025-05-07 10:36:33,164 - INFO - Successfully extracted 4602 characters from ./data\\III241.pdf\n",
      "2025-05-07 10:36:33,164 - INFO - Created 8 chunks from text\n",
      "2025-05-07 10:36:33,326 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:33,367 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:33,368 - INFO - Successfully added 8 chunks to ChromaDB\n",
      "2025-05-07 10:36:33,368 - INFO - Processing PDF 18/34: III250(A).pdf\n",
      "2025-05-07 10:36:33,378 - INFO - Reading ./data\\III250(A).pdf with 2 pages\n",
      "2025-05-07 10:36:33,395 - INFO - Successfully extracted 3398 characters from ./data\\III250(A).pdf\n",
      "2025-05-07 10:36:33,396 - INFO - Created 6 chunks from text\n",
      "2025-05-07 10:36:33,617 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:33,661 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:33,661 - INFO - Successfully added 6 chunks to ChromaDB\n",
      "2025-05-07 10:36:33,662 - INFO - Processing PDF 19/34: III300(A).pdf\n",
      "2025-05-07 10:36:33,670 - INFO - Reading ./data\\III300(A).pdf with 1 pages\n",
      "2025-05-07 10:36:33,685 - INFO - Successfully extracted 3218 characters from ./data\\III300(A).pdf\n",
      "2025-05-07 10:36:33,685 - INFO - Created 5 chunks from text\n",
      "2025-05-07 10:36:33,972 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:34,065 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:34,065 - INFO - Successfully added 5 chunks to ChromaDB\n",
      "2025-05-07 10:36:34,066 - INFO - Processing PDF 20/34: III300.pdf\n",
      "2025-05-07 10:36:34,074 - INFO - Reading ./data\\III300.pdf with 2 pages\n",
      "2025-05-07 10:36:34,102 - INFO - Successfully extracted 3870 characters from ./data\\III300.pdf\n",
      "2025-05-07 10:36:34,102 - INFO - Created 6 chunks from text\n",
      "2025-05-07 10:36:34,245 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:34,277 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:34,278 - INFO - Successfully added 6 chunks to ChromaDB\n",
      "2025-05-07 10:36:34,278 - INFO - Processing PDF 21/34: III400(C).pdf\n",
      "2025-05-07 10:36:34,287 - INFO - Reading ./data\\III400(C).pdf with 3 pages\n",
      "2025-05-07 10:36:34,311 - INFO - Successfully extracted 6069 characters from ./data\\III400(C).pdf\n",
      "2025-05-07 10:36:34,311 - INFO - Created 10 chunks from text\n",
      "2025-05-07 10:36:34,612 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:34,797 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:34,797 - INFO - Successfully added 10 chunks to ChromaDB\n",
      "2025-05-07 10:36:34,798 - INFO - Processing PDF 22/34: III400.pdf\n",
      "2025-05-07 10:36:34,807 - INFO - Reading ./data\\III400.pdf with 7 pages\n",
      "2025-05-07 10:36:34,977 - INFO - Successfully extracted 16088 characters from ./data\\III400.pdf\n",
      "2025-05-07 10:36:34,978 - INFO - Created 24 chunks from text\n",
      "2025-05-07 10:36:35,322 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:35,435 - INFO - Added batch 1/2 to ChromaDB\n",
      "2025-05-07 10:36:36,282 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:36,322 - INFO - Added batch 2/2 to ChromaDB\n",
      "2025-05-07 10:36:36,322 - INFO - Successfully added 24 chunks to ChromaDB\n",
      "2025-05-07 10:36:36,323 - INFO - Processing PDF 23/34: III510.pdf\n",
      "2025-05-07 10:36:36,332 - INFO - Reading ./data\\III510.pdf with 1 pages\n",
      "2025-05-07 10:36:36,356 - INFO - Successfully extracted 2865 characters from ./data\\III510.pdf\n",
      "2025-05-07 10:36:36,356 - INFO - Created 5 chunks from text\n",
      "2025-05-07 10:36:36,518 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:36,558 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:36,559 - INFO - Successfully added 5 chunks to ChromaDB\n",
      "2025-05-07 10:36:36,559 - INFO - Processing PDF 24/34: III600(A).pdf\n",
      "2025-05-07 10:36:36,568 - INFO - Reading ./data\\III600(A).pdf with 1 pages\n",
      "2025-05-07 10:36:36,578 - INFO - Successfully extracted 1203 characters from ./data\\III600(A).pdf\n",
      "2025-05-07 10:36:36,579 - INFO - Created 3 chunks from text\n",
      "2025-05-07 10:36:36,751 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:36,783 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:36,784 - INFO - Successfully added 3 chunks to ChromaDB\n",
      "2025-05-07 10:36:36,784 - INFO - Processing PDF 25/34: III600(B).pdf\n",
      "2025-05-07 10:36:36,793 - INFO - Reading ./data\\III600(B).pdf with 1 pages\n",
      "2025-05-07 10:36:36,804 - INFO - Successfully extracted 1848 characters from ./data\\III600(B).pdf\n",
      "2025-05-07 10:36:36,806 - INFO - Created 4 chunks from text\n",
      "2025-05-07 10:36:36,945 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:36,975 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:36,975 - INFO - Successfully added 4 chunks to ChromaDB\n",
      "2025-05-07 10:36:36,976 - INFO - Processing PDF 26/34: III600.pdf\n",
      "2025-05-07 10:36:36,985 - INFO - Reading ./data\\III600.pdf with 1 pages\n",
      "2025-05-07 10:36:36,997 - INFO - Successfully extracted 1943 characters from ./data\\III600.pdf\n",
      "2025-05-07 10:36:36,997 - INFO - Created 4 chunks from text\n",
      "2025-05-07 10:36:37,132 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:37,164 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:37,164 - INFO - Successfully added 4 chunks to ChromaDB\n",
      "2025-05-07 10:36:37,165 - INFO - Processing PDF 27/34: III610.pdf\n",
      "2025-05-07 10:36:37,173 - INFO - Reading ./data\\III610.pdf with 1 pages\n",
      "2025-05-07 10:36:37,178 - INFO - Successfully extracted 731 characters from ./data\\III610.pdf\n",
      "2025-05-07 10:36:37,179 - INFO - Created 2 chunks from text\n",
      "2025-05-07 10:36:37,762 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:37,838 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:37,839 - INFO - Successfully added 2 chunks to ChromaDB\n",
      "2025-05-07 10:36:37,839 - INFO - Processing PDF 28/34: III620(A).pdf\n",
      "2025-05-07 10:36:37,849 - INFO - Reading ./data\\III620(A).pdf with 2 pages\n",
      "2025-05-07 10:36:37,869 - INFO - Successfully extracted 4249 characters from ./data\\III620(A).pdf\n",
      "2025-05-07 10:36:37,869 - INFO - Created 8 chunks from text\n",
      "2025-05-07 10:36:38,150 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:38,195 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:38,196 - INFO - Successfully added 8 chunks to ChromaDB\n",
      "2025-05-07 10:36:38,196 - INFO - Processing PDF 29/34: III620(B).pdf\n",
      "2025-05-07 10:36:38,206 - INFO - Reading ./data\\III620(B).pdf with 1 pages\n",
      "2025-05-07 10:36:38,219 - INFO - Successfully extracted 3088 characters from ./data\\III620(B).pdf\n",
      "2025-05-07 10:36:38,219 - INFO - Created 5 chunks from text\n",
      "2025-05-07 10:36:38,622 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:38,795 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:38,796 - INFO - Successfully added 5 chunks to ChromaDB\n",
      "2025-05-07 10:36:38,796 - INFO - Processing PDF 30/34: III630(A).pdf\n",
      "2025-05-07 10:36:38,806 - INFO - Reading ./data\\III630(A).pdf with 7 pages\n",
      "2025-05-07 10:36:38,878 - INFO - Successfully extracted 22062 characters from ./data\\III630(A).pdf\n",
      "2025-05-07 10:36:38,878 - INFO - Created 32 chunks from text\n",
      "2025-05-07 10:36:39,381 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:39,437 - INFO - Added batch 1/2 to ChromaDB\n",
      "2025-05-07 10:36:40,334 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:40,629 - INFO - Added batch 2/2 to ChromaDB\n",
      "2025-05-07 10:36:40,629 - INFO - Successfully added 32 chunks to ChromaDB\n",
      "2025-05-07 10:36:40,630 - INFO - Processing PDF 31/34: III640(A).pdf\n",
      "2025-05-07 10:36:40,639 - INFO - Reading ./data\\III640(A).pdf with 1 pages\n",
      "2025-05-07 10:36:40,648 - INFO - Successfully extracted 824 characters from ./data\\III640(A).pdf\n",
      "2025-05-07 10:36:40,648 - INFO - Created 2 chunks from text\n",
      "2025-05-07 10:36:40,884 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:40,925 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:40,925 - INFO - Successfully added 2 chunks to ChromaDB\n",
      "2025-05-07 10:36:40,925 - INFO - Processing PDF 32/34: III700(A).pdf\n",
      "2025-05-07 10:36:40,935 - INFO - Reading ./data\\III700(A).pdf with 4 pages\n",
      "2025-05-07 10:36:40,973 - INFO - Successfully extracted 12675 characters from ./data\\III700(A).pdf\n",
      "2025-05-07 10:36:40,974 - INFO - Created 19 chunks from text\n",
      "2025-05-07 10:36:41,186 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:41,257 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:41,257 - INFO - Successfully added 19 chunks to ChromaDB\n",
      "2025-05-07 10:36:41,257 - INFO - Processing PDF 33/34: III800(A).pdf\n",
      "2025-05-07 10:36:41,267 - INFO - Reading ./data\\III800(A).pdf with 1 pages\n",
      "2025-05-07 10:36:41,281 - INFO - Successfully extracted 4219 characters from ./data\\III800(A).pdf\n",
      "2025-05-07 10:36:41,282 - INFO - Created 7 chunks from text\n",
      "2025-05-07 10:36:41,567 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:41,756 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:41,756 - INFO - Successfully added 7 chunks to ChromaDB\n",
      "2025-05-07 10:36:41,757 - INFO - Processing PDF 34/34: III900(A).pdf\n",
      "2025-05-07 10:36:41,765 - INFO - Reading ./data\\III900(A).pdf with 2 pages\n",
      "2025-05-07 10:36:41,786 - INFO - Successfully extracted 6226 characters from ./data\\III900(A).pdf\n",
      "2025-05-07 10:36:41,786 - INFO - Created 11 chunks from text\n",
      "2025-05-07 10:36:42,467 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-05-07 10:36:42,582 - INFO - Added batch 1/1 to ChromaDB\n",
      "2025-05-07 10:36:42,583 - INFO - Successfully added 11 chunks to ChromaDB\n",
      "2025-05-07 10:36:42,748 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 34 PDF files\n"
     ]
    }
   ],
   "source": [
    "pipeline = PDFEmbeddingPipeline(\n",
    "    openai_api_key=api_key,\n",
    "    chroma_db_path=\"./chroma_db\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# Process an entire directory\n",
    "results = pipeline.process_directory(\"./data\")\n",
    "print(f\"Processed {len(results)} PDF files\")\n",
    "\n",
    "# Query the database\n",
    "similar_chunks = pipeline.query(\"What is machine learning?\", n_results=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
